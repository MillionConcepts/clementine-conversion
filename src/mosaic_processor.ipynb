{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aquatic-footwear",
   "metadata": {},
   "source": [
    "# notebook for converting clementine mosaic files in bulk\n",
    "\n",
    "## notes and performance tips:\n",
    "1. other than selecting a source index, everything necessary to distinguish\n",
    "mosaic product types and handle their individual idiosyncracies is\n",
    "handled by ```clem_conversion.ClemMosaicConverter```. There isn't a bunch of\n",
    "preprocessing in this notebook, and no need for anything like the EDR decompression\n",
    "notebook. The Clementine mosaics are all pretty similar to one another, each product\n",
    "only has one data object, and they don't need to be decompressed for the\n",
    "'planetary data stack' (```pvl, pdr, gdal```, etc.) to work on them.\n",
    "\n",
    "2. CPU, throughput, and IOPS (the last especially for HIRES) are all plausible\n",
    "bottlenecks. We strongly recommend parallelizing this, perhaps just by running\n",
    "duplicates of this notebook n parallel.\n",
    "\n",
    "3. ```ClemMosaicConverter``` makes no special effort to be careful about\n",
    "memory, because even the largest Clementine tiles are well under 100 MB. However,\n",
    "if you are running this on a memory-constrained machine, you might want\n",
    "to hack it a bit.\n",
    "\n",
    "4. We're never thinking about distinct PDS3 label files: all of the tiles have\n",
    "attached PDS3 labels. Note that there are also detached ISIS qube files in the\n",
    "same directories; while this isn't important for the current writer configuration,\n",
    "if you later want to add ```rasterio``` into this stack, it might be.\n",
    "\n",
    "5. This does not handle the 56 HIRES mosaic products with duplicated product IDs.\n",
    "See [dupe_hires_tile_fix.ipynb](dupe_hires_tile_fix.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import fs.path\n",
    "from fs.osfs import OSFS\n",
    "from more_itertools import distribute\n",
    "import pandas as pd\n",
    "import sh\n",
    "\n",
    "from clem_bulk import crude_time_log, swap_lat_and_scale\n",
    "from clem_conversion import ClemMosaicConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set root directories for your input and output data sets\n",
    "input_fs = OSFS('/home/ubuntu/buckets/clem_input/')\n",
    "output_fs = OSFS('/home/ubuntu/buckets/clem_output/')\n",
    "# we wrote these directly to an s3fs-fuse filesystem.\n",
    "# gdal does not deal well with writing directly to s3 filesystems,\n",
    "# for unclear reasons. this roundabout temp directory step\n",
    "# would serve no purpose in another configuration.\n",
    "temp_output_directory = '/home/ubuntu/data_temp/'\n",
    "\n",
    "# our manifest of 'standard' mosaic products (specifically not counting some\n",
    "# of the oddball / mangled basemap files on cl_3015), along with mappings to\n",
    "# their new paths\n",
    "mosaic_manifest = pd.read_csv(\n",
    "    './directories/clementine/standard_mosaic_products.csv'\n",
    ")\n",
    "\n",
    "# associate hires source index with an index of edr products that includes time\n",
    "# in order to determine start and stop times for each hires mosaic tile\n",
    "hires_source_groupby = pd.read_csv(\n",
    "    './directories/clementine/hires_source_index.csv',\n",
    ").groupby('tilename')\n",
    "\n",
    "# do the same for basemap \n",
    "# (note that indices of this type are not available for uvvis and nir mosaics)\n",
    "basemap_source_groupby = pd.read_csv(\n",
    "    './directories/clementine/basemap_source_index.csv',\n",
    ").groupby('tilename')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a simple method of splitting your index up if\n",
    "# you're parallelizing across multiple notebooks / machines /\n",
    "# whatever.\n",
    "\n",
    "# chunk tiles into separate lists\n",
    "tile_chunks = distribute(25, mosaic_manifest.itertuples())\n",
    "\n",
    "# increment this next variable (serially or in parallel) from 0 to 49\n",
    "# across a series of notebooks / scripts / whatever in order to convert\n",
    "# all of the tiles in an organized fashion\n",
    "tile_chunk_index = 0\n",
    "tile_chunk = list(tile_chunks[tile_chunk_index]) # greedily evaluate to get the length\n",
    "chunk_length = len(tile_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-spotlight",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert all the files in this chunk \n",
    "for ix, tile in enumerate(tile_chunk):\n",
    "    print(\"Converting \" + fs.path.split(tile.file)[1])\n",
    "    print(str(ix) + \" of \" + str(chunk_length) + \" in this chunk.\")\n",
    "    \n",
    "    tile_start_time = dt.datetime.now() # just for logging\n",
    "    \n",
    "    source_path = tile.file\n",
    "    # select source product index (this is basically used only to write \n",
    "    # detailed start / stop time tags in basemap and hires)\n",
    "    if fs.path.split(source_path)[1].startswith('b'):\n",
    "        source_groups = basemap_source_groupby\n",
    "    elif fs.path.split(source_path)[1].startswith(('h', 'g')):\n",
    "        source_groups = hires_source_groupby\n",
    "    else:\n",
    "        source_groups = None\n",
    "    \n",
    "    destination_path = tile.newpath\n",
    "    output_fs.makedirs(destination_path, recreate=True)\n",
    "    \n",
    "    # initialize writer & convert product \n",
    "    writer = ClemMosaicConverter(\n",
    "        input_fs.getsyspath(source_path),\n",
    "        source_groups = source_groups\n",
    "    )\n",
    "    writer.write_pds4(temp_output_directory)\n",
    "    \n",
    "    # this is a hacky fix for an incompatibility between\n",
    "    # gdal and the current version of the cart LDD\n",
    "    if \"hires_polar\" in writer.pds4_root:\n",
    "        swap_lat_and_scale(temp_output_directory + writer.pds4_root + \".xml\")\n",
    "\n",
    "    for extension in ['.xml', '.tif']:\n",
    "        sh.mv(\n",
    "            temp_output_directory + writer.pds4_root + extension, \n",
    "            output_fs.getsyspath(destination_path),\n",
    "            _bg=True\n",
    "        )\n",
    "\n",
    "    # very simple logger\n",
    "    # probably better to distinguish log filenames if you're running\n",
    "    # these concurrently on the same machine\n",
    "    elapsed = str((dt.datetime.now() - tile_start_time).total_seconds())\n",
    "    crude_time_log(\n",
    "        'mosaic_conversion_log_' + str(tile_chunk_index) + '.csv',\n",
    "        writer,\n",
    "        elapsed\n",
    "    )\n",
    "    print(\"total seconds: \" + elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}