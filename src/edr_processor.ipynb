{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "constitutional-digit",
   "metadata": {},
   "source": [
    "# notebook for converting Clementine EDR files\n",
    "\n",
    "**IMPORTANT:**\n",
    "This notebook expects to be working with *decompressed* files. It is\n",
    "impractical to decompress files one-by-one from CLEM-JPEG during the\n",
    "PDS3 -> PDS4 conversion step. See clemdcmp_processor.ipynb for a bulk\n",
    "decompression method.\n",
    "\n",
    "## performance tips\n",
    "\n",
    "IOPS is the most likely bottleneck, but CPU is also possible. We recommend\n",
    "both using a fast disk and parallelizing this, either using ```pathos```\n",
    "(vanilla Python ```multiprocessing``` will fail during a pickling step)\n",
    "or simply by running multiple copies of this notebook.\n",
    "\n",
    "Also note that our manifest file is pretty large file (~1.4 GB), as it includes\n",
    "a lot of metadata (here used primarily to read compression stats).\n",
    "it might be worth reading it in chunks, dropping chunks of it\n",
    "(as this notebook does), or paring it down in some way if you're running\n",
    "this many times in parallel or on a memory-constrained computer.\n",
    "This is really the only constraint on working memory, because the individual\n",
    "EDR files are so small.\n",
    "\n",
    "## corrupted files\n",
    "\n",
    "A handful of files in the PDS3 archive appear to be corrupt. CLEMDCMP\n",
    "produces visually-ok output that is nevertheless unparseable by any\n",
    "method we attempted. The ```CANNOT_PARSE_FILES``` variable below marks\n",
    "these files for skipping so that the pipeline doesn't choke on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "from fs.osfs import OSFS\n",
    "import pandas as pd\n",
    "\n",
    "from clem_bulk import crude_time_log\n",
    "from clem_conversion import ClemEDRConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANNOT_PARSE_FILES = [\n",
    "    'lla0807f.222', 'lla3440m.230', 'lla2912l.202', 'lla0977f.210', 'lla1744i.174',\n",
    "    'lna5725y.034', 'lla3357m.226', 'lhb0557k.323', 'lla2996l.254', 'lla1355g.279',\n",
    "    'lla2236j.260', 'lla1914s.341', 'lla2320j.268', 'lla1999i.280', 'lla2167j.274' \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to your input and output filesystems\n",
    "input_fs = OSFS('/home/ubuntu/buckets/clemdcmp_holding/')\n",
    "output_fs = OSFS('/home/ubuntu/buckets/clem_output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of file metadata + our new output paths.\n",
    "# the metadata is really used just to populate onboard compression\n",
    "# values, because CLEMDCMP strips them from its output labels and\n",
    "# parsing both the compressed and decompressed files at this stage\n",
    "# is a pain.\n",
    "imindex = pd.read_csv('./directories/clementine/clementine_edr_index.csv')\n",
    "# uppercasing this for parity with labels\n",
    "# (the PDS3 archive is not very consistent about capitalization)\n",
    "imindex['product_id'] = imindex['product_id'].str.upper() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is a place to split your index up if you're\n",
    "# parallelizing across multiple notebooks.\n",
    "from more_itertools import chunked\n",
    "chunk_ix_of_this_notebook = 0\n",
    "chunk_size = 100000\n",
    "chunks = chunked(imindex.index, chunk_size)\n",
    "chunk_ix = 0\n",
    "while chunk_ix <= chunk_ix_of_this_notebook:\n",
    "    chunk = next(chunks)\n",
    "    chunk_ix += 1\n",
    "working_index = imindex.loc[chunks].copy()\n",
    "del imindex\n",
    "log_string = \"_\" + str(chunk_ix_of_this_notebook)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just to quickly break and restart if odd things happen.\n",
    "try: \n",
    "    old_ix = ix\n",
    "except NameError:\n",
    "    old_ix = 0\n",
    "\n",
    "for ix, edr_image in enumerate(working_index.itertuples()):\n",
    "    if ix < old_ix:\n",
    "        continue\n",
    "    if ix % 250 == 0:\n",
    "        print(\"Converting \" + edr_image.product_id.lower())\n",
    "        print(str(ix) + \" of \" + str(len(working_index)) + \" in this volume.\")    \n",
    "    if edr_image.product_id.lower() in CANNOT_PARSE_FILES:\n",
    "        print('skipping known bad file ' + edr_image.product_id.lower())\n",
    "        continue\n",
    "    edr_start_time = dt.datetime.now() # just for logging\n",
    "    # note that clem_conversion.clemdcmp() adds an .img extension\n",
    "    # to differentiate decompressed files from their compressed sources,\n",
    "    # and the decompression notebook provided simply splits files into\n",
    "    # directories by archive volume number (0001 - 0088). If you have\n",
    "    # arranged them some other way, change this.\n",
    "    source_path = str(\n",
    "        edr_image.volume_id + '/' + edr_image.product_id + '.img'\n",
    "    ).lower()\n",
    "    destination_path = edr_image.newpath\n",
    "    output_fs.makedirs(destination_path, recreate=True)\n",
    "\n",
    "    # initialize writer & convert product\n",
    "    try: \n",
    "        writer = ClemEDRConverter(\n",
    "            source_path,\n",
    "            image_index = working_index,\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\n",
    "            \"OH NO! ACK!  SOMETHING IS WRONG WITH THIS \\n \\n \\n FILE!!!!!!!\",\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"ACK!!!!\\n\",\n",
    "        )\n",
    "        print(source_path + \" is bad!!!!!!!\"),\n",
    "        print(\"\\n\\n\\n\"),\n",
    "        print(\"*************************)\")\n",
    "        with open('failed_edr_files' + log_string + '.txt') as file:\n",
    "            file.write(source_path)\n",
    "        continue\n",
    "    writer.write_pds4(output_fs.getsyspath('') + destination_path, verbose = False)\n",
    "\n",
    "    # very simple logger\n",
    "    # probably better to distinguish log filenames if you're running\n",
    "    # these concurrently on the same machine\n",
    "    elapsed = str((dt.datetime.now() - edr_start_time).total_seconds())\n",
    "    crude_time_log(\n",
    "        'edr_conversion_log_' + log_string + '.csv',\n",
    "        writer,\n",
    "        elapsed\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}